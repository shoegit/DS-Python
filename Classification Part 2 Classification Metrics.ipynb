{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Classification Part 2 Classification Metrics\n",
    "\n",
    "_Authors: Matt Brems, Dave Yerrington, Noelle Brown, Jeff Hale, Ng Shu Min_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Calculate various classification model evaluation metrics.\n",
    "- Describe the inverse relationship between sensitivity and specificity.\n",
    "- Understand what the ROC shows and interpret ROC AUC.\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "---\n",
    "\n",
    "We'll need the following libraries for this notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import train_test_split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Logistic Regression model.\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import metrics.\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score, plot_roc_curve, roc_auc_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Our Model\n",
    "\n",
    "We will load in the model that we saved previously into the pickle file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'admissions_log_reg.pkl'\n",
    "\n",
    "#unpacking pickle file\n",
    "with open(filename, 'rb') as f:\n",
    "    lr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the X_test and y_test that we saved\n",
    "X_test = pd.read_csv('admissions_X_test.csv', index_col=0)\n",
    "\n",
    "# Similarly load in y_test. What is the difference when the index_col is not specified?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the model on the test set\n",
    "# What does the result represent?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "\n",
    "The `score` method returns the accuracy, but there are other important classification metrics that we need.\n",
    "\n",
    "First we should store the predictions that we obtained and have a look at them more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the predictions in a variable y_hat\n",
    "y_hat = lr.predict(X_test)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probability that admit=1\n",
    "pred_probs = lr.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_results = pd.DataFrame({'gpa':X_test['gpa'], \n",
    "                                  'actual':y_test['admit'], \n",
    "                                  'predicted':y_hat,\n",
    "                                  'pred_probs':pred_probs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the rows where the actual is not equal to predicted\n",
    "predicted_results[predicted_results['actual']!=predicted_results['predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about where the actual is  equal to predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many actual = predicted\n",
    "num_correct = predicted_results[predicted_results['actual']==predicted_results['predicted']].shape[0]\n",
    "num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many actual != predicted\n",
    "num_wrong = predicted_results[predicted_results['actual']!=predicted_results['predicted']].shape[0]\n",
    "num_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy\n",
    "accuracy = num_correct / (num_correct + num_wrong)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Instead of manually calculating the true positive and true negatives, we can get the confusion matrix from `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion DataFrame\n",
    "---\n",
    "\n",
    "The confusion matrix we just created isn't very explanatory, so let's drop it into a pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm, columns=['pred 0', 'pred 1'], index=['actual 0', 'actual 1'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy\n",
    "# (TP + TN) / (Total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also find using sklearn.metrics function\n",
    "accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many Type I errors are there?\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary>Need a hint?</summary>\n",
    "    Type I = False positive\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many Type II errors are there?\n",
    "---\n",
    "<details>\n",
    "    <summary>Need a hint?</summary>\n",
    "    Type II = False negatives\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate recall (a.k.a. sensitivity)\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary>Need a hint?</summary>\n",
    "    Recall = Sensitivity, and there are no p's in sensitivity: TP/P\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check using sklearn.metric's recall_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the specificity\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary>Need a hint?</summary>\n",
    "    There is a p in specificity, therefore there are no p's in the calculation: TN/all N\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the precision\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary>Need a hint?</summary>\n",
    "    Precision is number true positives in positive predictions: TN / (TP + FP)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn.metric precision_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall.\n",
    "\n",
    "If you care about precision and recall roughly the same amount, F1 score is a great metric to use.\n",
    "\n",
    "Note that even though all the metrics you‚Äôve seen can be followed by the word score F1 always is. ü§∑‚Äç‚ôÄÔ∏è\n",
    "\n",
    "$$\n",
    "2*\\frac{(\\text{Precision}*\\text{Recall})} {(\\text{Precision} + \\text{Recall})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the F1 Score?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by hand\n",
    "2 * (precision*recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Confusion Matrix Tips and Tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it!\n",
    "# https://github.com/justmarkham/scikit-learn-tips/blob/master/notebooks/20_plot_confusion_matrix.ipynb\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(lr, X_test, y_test, cmap='Blues', \n",
    "                      values_format='d', display_labels=['admit = 0', 'admit = 1']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ravel it!\n",
    "# Save TN/FP/FN/TP values.\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_hat).ravel()\n",
    "\n",
    "# Note that .ravel() will arrange items in a one-dimensional array.\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between Sensitivity and Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a plot to compare the predicted probabilities and the actual results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "sns.histplot(predicted_results, x='pred_probs', hue='actual', bins=20, alpha=0.2)\n",
    "plt.xlabel('Predicted Probability that Outcome = 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where there are overlaps, there would be misclassifications of some of the observations.\n",
    "\n",
    "Consider if the cutoff for the predicted probability is 0.5, so an observation with predicted probability 0.5 and above would be predicted as 1, and below 0.5 would be predicted as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "# plot distributions of predicted probabilities by actual values\n",
    "sns.histplot(predicted_results, x='pred_probs', hue='actual', bins=20, alpha=0.2)\n",
    "\n",
    "# Add cutoff line\n",
    "plt.axvline(0.5, color = 'black', linestyle = '--')\n",
    "\n",
    "plt.xlabel('Predicted Probability that Outcome = 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "# plot distributions of predicted probabilities by actual values\n",
    "sns.histplot(predicted_results, x='pred_probs', hue='actual', bins=20, alpha=0.2)\n",
    "\n",
    "# Add cutoff line\n",
    "plt.axvline(0.5, color = 'black', linestyle = '--')\n",
    "\n",
    "# Add annotations for TN, FN, TP, FP.\n",
    "plt.annotate(xy = (0.0, 100), text = 'TN', size = 15)\n",
    "plt.annotate(xy = (0.4, 5), text = 'FN', size = 15)\n",
    "plt.annotate(xy = (0.90, 1), text = 'TP', size = 15)\n",
    "plt.annotate(xy = (0.65, 10), text = 'FP', size = 15)\n",
    "\n",
    "plt.xlabel('Predicted Probability that Outcome = 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "# plot distributions of predicted probabilities by actual values\n",
    "sns.histplot(predicted_results, x='pred_probs', hue='actual', bins=20, alpha=0.2)\n",
    "\n",
    "# Add cutoff line\n",
    "plt.axvline(0.3, color = 'black', linestyle = '--')\n",
    "\n",
    "# Add annotations for TN, FN, TP, FP.\n",
    "plt.annotate(xy = (0.0, 100), text = 'TN', size = 15)\n",
    "plt.annotate(xy = (0.1, 1), text = 'FN', size = 15)\n",
    "plt.annotate(xy = (0.90, 1), text = 'TP', size = 15)\n",
    "plt.annotate(xy = (0.3, 10), text = 'FP', size = 15)\n",
    "\n",
    "\n",
    "plt.xlabel('Predicted Probability that Outcome = 1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>When I moved my classification threshold to the left, what happened to sensitivity and specificity?</summary>\n",
    "- Our number of true negatives decreased and our number of total negatives remains the same.\n",
    "    - $\\text{Specificity} = \\frac{TN}{N} \\Rightarrow \\text{Specificity decreases.}$\n",
    "- Our number of true positives increased and our number of total positives remains the same.\n",
    "    - $\\text{Sensitivity} = \\frac{TP}{P} \\Rightarrow \\text{Sensitivity increases.}$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal should be to build a model such that there is **no overlap** between the blue histogram and the orange histogram!\n",
    "- If there is overlap, we need to recognize the tradeoff between sensitivity and specificity. (As one increases, the other decreases.)\n",
    "- One measure of how much overlap exists between our distributions is the **area under the ROC curve**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "Plot the True Positive Rate vs. False Positive Rate for the range of possible decision thresholds and you get the ROC curve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(lr, X_test, y_test)\n",
    "plt.plot([0, 1], [0, 1],\n",
    "         label='baseline', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC\n",
    "\n",
    "The more area under this blue curve is, the better separated our distributions are.\n",
    "- Check out this gif ([source](https://twitter.com/DrHughHarvey/status/1104435699095404544)):\n",
    "\n",
    "![](https://media.giphy.com/media/H1SZ5oRLIuZ1t1c4Di/giphy.gif)\n",
    "\n",
    "We use the **area under the ROC curve** (abbreviated **ROC AUC** or **AUC ROC**) to quantify the gap between our distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting ROC AUC\n",
    "- If you have an ROC AUC of 0.5, your positive and negative populations perfectly overlap and your model is as bad as it can get.\n",
    "- If you have an ROC AUC of 1, your positive and negative populations are perfectly separated and your model is as good as it can get.\n",
    "- The closer your ROC AUC is to 1, the better. (1 is the maximum score.)\n",
    "- If you have an ROC AUC of below 0.5, your positive and negative distributions have flipped sides. By flipping your predicted values (i.e. flipping predicted 1s and 0s), your ROC AUC will now be above 0.5.\n",
    "    - Example: You have an ROC AUC of 0.2. If you change your predicted 1s to 0s and your predicted 0s to 1s, your ROC AUC will now be 0.8!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate one ROC curve per model. The ROC curve is generated by varying our threshold from 0 to 1. This doesn't actually change the threshold or our original predictions, but it helps us to visualize our tradeoff between _sensitivity_ and _specificity_ and understand how well-separated our populations are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing your prediction threshold\n",
    "\n",
    "If you want, you could change your prediction threshold to a custom value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(probabilities, threshold):\n",
    "    return [0 if prob < threshold else 1 for prob in probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_predictions(lr.predict_proba(X_test)[:,1], 0.3)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_list = [0.25, 0.4, 0.5, 0.6, 0.75]\n",
    "\n",
    "for threshold in threshold_list:\n",
    "    preds = get_predictions(lr.predict_proba(X_test)[:,1], threshold)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f'Threshold: {threshold}, Accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Classification metrics are used to evaluate a classification model. \n",
    "\n",
    "We can generate a confusion matrix to view the True Positives, True Negatives, False Positives and False Negatives.\n",
    "\n",
    "Then we use these values to calculate \n",
    "- accuracy (1 - misclassification)\n",
    "- recall (aka sensitivity or true positive rate)\n",
    "- specificity (1 - false positive rate)\n",
    "- precision (positive predictive value)\n",
    "\n",
    "For a binary classifier, the naive classifer would be based on classifying at random (0.5 chance of getting 1 or 0), so our model should have an accuracy that is better than 50%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
